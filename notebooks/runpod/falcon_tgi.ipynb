{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyledinh/gpt-prive/blob/main/notebooks/runpod/falcon_tgi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X8SUHvMvTVS"
      },
      "source": [
        "References: \n",
        "\n",
        "- https://github.com/huggingface/text-generation-inference\n",
        "- https://vilsonrodrigues.medium.com/serving-falcon-models-with-text-generation-inference-tgi-5f32005c663b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### WORKING NOTES:\n",
        "\n",
        "- https://github.com/huggingface/text-generation-inference#api-documentation\n",
        "\n",
        "```\n",
        "model=meta-llama/Llama-2-7b-chat-hf\n",
        "volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n",
        "token=<your cli READ token>\n",
        "\n",
        "docker run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:0.9.3 --model-id $model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QNAfi-140YI"
      },
      "source": [
        "## On-premise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0j0elnkvMiA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "docker run --gpus all --shm-size 1g -p 8080:80 -v $PWD/data:/data \n",
        "     ghcr.io/huggingface/text-generation-inference:0.8 \\ \n",
        "     --model-id tiiuae/falcon-7b-instruct \\ \n",
        "     --num-shard 1  \\ \n",
        "     --quantize bitsandbytes     \n",
        "\"\"\"     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzdxAQ2zJ--k"
      },
      "source": [
        "Bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SfCo-sBJ-hI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "curl 127.0.0.1:8080/generate \\\n",
        "     -X POST \\\n",
        "     -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":17}}' \\\n",
        "     -H 'Content-Type: application/json'\n",
        "\n",
        "curl 127.0.0.1:8080/generate_stream \\\n",
        "    -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":17}}' \\\n",
        "    -H 'Content-Type: application/json'\n",
        "\n",
        "curl 127.0.0.1:8080/ \\\n",
        "    -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\n",
        "          \"parameters\":{\"max_new_tokens\":17},\n",
        "          \"stream\": True}' \\\n",
        "    -H 'Content-Type: application/json'\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhN-cYFYKWWx"
      },
      "source": [
        "TGI Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFaVKp8YKU34"
      },
      "outputs": [],
      "source": [
        "%pip install text-generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh9H_kRyKf9h"
      },
      "outputs": [],
      "source": [
        "from text_generation import Client\n",
        "\n",
        "# Generate\n",
        "client = Client(\"http://127.0.0.1:8080\")\n",
        "print(client.generate(\"What is Deep Learning?\", max_new_tokens=17).generated_text)\n",
        "\n",
        "# Generate stream\n",
        "text = \"\"\n",
        "for response in client.generate_stream(\"What is Deep Learning?\", max_new_tokens=17):\n",
        "    if not response.token.special:\n",
        "        text += response.token.text\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHGXzzXCKi1j"
      },
      "source": [
        "LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmrkZksFKkbM"
      },
      "outputs": [],
      "source": [
        "%pip install langchain transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM7Vi4P8K6mB"
      },
      "outputs": [],
      "source": [
        "# Wrapper to TGI client with langchain\n",
        "\n",
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "\n",
        "inference_server_url_local = \"http://127.0.0.1:8080\"\n",
        "\n",
        "llm_local = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_server_url_local,\n",
        "    max_new_tokens=1000,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHWNDoZaMXSp"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template, \n",
        "    input_variables= [\"question\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm_local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxTX3BohMjAO"
      },
      "outputs": [],
      "source": [
        "llm_chain(\"your question\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awxPhMTFMsXW"
      },
      "source": [
        "## Run Pod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4WBYp63Mw0t"
      },
      "outputs": [],
      "source": [
        "%pip install runpod python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8s8n1SiCVw5P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HF_ACCESS_TOKEN configured\n",
            "RUNPOD_API_KEY configured\n"
          ]
        }
      ],
      "source": [
        "import runpod\n",
        "# Load .env variables\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv())\n",
        "\n",
        "HF_ACCESS_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\", \"add-here-if-not-set-in-env-file\")\n",
        "RUNPOD_API_KEY = os.getenv(\"RUNPOD_API_KEY\", \"add-here-if-not-set-in-env-file\")\n",
        " \n",
        "assert HF_ACCESS_TOKEN.startswith(\"hf_\"), \"This doesn't look like a valid Hugging Face Token\"\n",
        "print(\"HF_ACCESS_TOKEN configured\")\n",
        "assert not RUNPOD_API_KEY.startswith(\"add-here\"), \"This doesn't look like a valid Runpod API Key\"\n",
        "print(\"RUNPOD_API_KEY configured\")\n",
        "\n",
        "# your key\n",
        "runpod.api_key = RUNPOD_API_KEY "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vz0BQ8BLW7S5"
      },
      "outputs": [],
      "source": [
        "num_shard = 1\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "quantize = \"bitsandbytes\"\n",
        "\n",
        "\n",
        "# https://docs.runpod.io/docs/create-pod (instruction in curl)\n",
        "# https://vilsonrodrigues.medium.com/serving-falcon-models-with-text-generation-inference-tgi-5f32005c663b (python)\n",
        "pod = runpod.create_pod(\n",
        "    name=\"Falcon-7B-Instruct-POD\",\n",
        "    image_name=\"ghcr.io/huggingface/text-generation-inference:0.8\",\n",
        "    gpu_type_id=\"NVIDIA GeForce RTX 4080\",\n",
        "    cloud_type=\"COMMUNITY\",\n",
        "    docker_args=f\"--model-id {model_id} --num-shard {num_shard} --quantize {quantize}\",\n",
        "    gpu_count=num_shard,\n",
        "    volume_in_gb=50,\n",
        "    container_disk_in_gb=5,\n",
        "    ports=\"80/http\",\n",
        "    volume_mount_path=\"/data\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BILLiwN_ZQBO"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "f-string: unmatched '[' (1963392774.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    inference_server_url_cloud = f\"https://{pod[\"id\"]}-80.proxy.runpod.net\"\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "\n",
        "inference_server_url_cloud = f\"https://{pod[\"id\"]}-80.proxy.runpod.net\"\n",
        "\n",
        "llm_cloud = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_server_url_cloud,\n",
        "    max_new_tokens=1000,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f821V3hrZjBu"
      },
      "outputs": [],
      "source": [
        "llm_chain_cloud = LLMChain(prompt=prompt, llm=llm_cloud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uim-6E2Gbh3K"
      },
      "outputs": [],
      "source": [
        "llm_chain_cloud(\"your new question to falcon\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLk0DvIXcl2O"
      },
      "outputs": [],
      "source": [
        "# stop pod\n",
        "runpod.stop_pod(pod[\"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6zLY333cotl"
      },
      "outputs": [],
      "source": [
        "# terminate\n",
        "runpod.terminate_pod(pod[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYxXY-Nz8mFG"
      },
      "source": [
        "## AWS Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljVgI3BU49V_"
      },
      "source": [
        "https://aws.amazon.com/pt/blogs/machine-learning/announcing-the-launch-of-new-hugging-face-llm-inference-containers-on-amazon-sagemaker/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNwwtzQ1UKz+cAIPVyarodR",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
