# Quantize LLM with GPTQ
> Make large LLM smaller with gptq 

## Steps
- target_model
- dataset
- quantization configuration
- push new model to HF hub

## References
- Hugging Face Blog Post [Making LLMs lighter with AutoGPTQ and transformers](https://huggingface.co/blog/gptq-integration)
- https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing
- Video [How To Convert LLMs to GPTQ models in 10 mins](https://www.youtube.com/watch?v=RlCQTtIYajM)
- https://www.youtube.com/watch?v=mii-xFaPCrA
